
%h1 Grand Challenges


%div{class: 'panel-group', id: 'accordion'}
	%div{class: 'panel panel-primary'}
		%div{class: 'panel-heading'}
			%h4{class: 'panel-title'}
				%a{'data-toggle': 'collapse', 'data-parent': '#accordion', href: '#gc-pcc'}
					Point Cloud Coding
				%span{class: 'glyphicon glyphicon-link pull-right clickable-workshop-link', ldata: 'gc-pcc'}
		%div{id: 'gc-pcc', class: 'panel-collapse collapse'}
			%div{class: 'panel-body'}
				%h4{class: 'specsess-heading'} Description
				%p With the significant growth of 3D sensing technologies point clouds have become a viable solution, since they provide practical ways for capture, storage, delivery and rendering in augmented reality, mixed reality, virtual reality, medical imaging and 3D printing applications, among others. There is a need for an interchange and delivery format allowing an efficient point cloud compression with minimal or no impact on their quality. This challenge solicits contributions for this purpose. Moreover, new evaluation methodologies are sought. Furthermore, additional publicly accessible point cloud content along with evidence for compression efficiency as well as other attractive features are also accepted.
				/ %h4{class: 'specsess-heading'} Important Dates
				/ %ul
				/ 	%li January 15, 2018: Grand Challenge proponents’ expression of interest
				/ 	%li February 15, 2018: Grand Challenge proponents’ submission
				/ 	%li March 1, 2018: Grand Challenge proponents’ optional paper submission
				/ 	%li March 15, 2018: Grand Challenge results paper submission
				/ 	%li March 31, 2018: Grand Challenge acceptance notification
				/ 	%li April 13, 2018: Grand Challenge camera-ready paper submission
				%h4{class: 'specsess-heading'} Website
				%a{href: 'https://mmspg.epfl.ch/ICME2018GrandChallenge'} https://mmspg.epfl.ch/ICME2018GrandChallenge
				%h4{class: 'specsess-heading'} Organizers
				= render 'chairs', target: :gc_pcc

	%div{class: 'panel panel-primary'}
		%div{class: 'panel-heading'}
			%h4{class: 'panel-title'}
				%a{'data-toggle': 'collapse', 'data-parent': '#accordion', href: '#gc-hfr'}
					Heterogeneous Face Recognition: Polarimetric Thermal-to-Visible Matching
				%span{class: 'glyphicon glyphicon-link pull-right clickable-workshop-link', ldata: 'gc-hfr'}
		%div{id: 'gc-hfr', class: 'panel-collapse collapse'}
			%div{class: 'panel-body'}
				%h4{class: 'specsess-heading'} Description
				%p This grand challenge is focused on heterogeneous face recognition, specifically on polarimetric thermal-to-visible matching. The motivation behind this challenge is the development of a nighttime face recognition capability for homeland security and defense. The challenge organizers will provide a polarimetric thermal and visible face database for algorithm development. Participants will be asked to provide heterogeneous face recognition algorithms in the form of executables, that take a pair of images (an aligned polarimetric thermal face image and an aligned visible face image) as input and provide a similarity score as output. Algorithms will be ranked by their face verification performance using ROCcurves.
				/ %h4{class: 'specsess-heading'} Important Dates
				/ %ul
				/ 	%li February 7, 2018: Executable files submitted to coordinators
				/ 	%li February 28, 2018: Winner announced
				/ 	%li March 15, 2018: (Optional) Winner paper submission
				/ 	%li March 31, 2018: Paper acceptance notification
				/ 	%li April 13, 2018: Camera-ready paper submission
				%h4{class: 'specsess-heading'} Website
				%a{href: 'https://sites.google.com/view/hfr-challenge18/home'} https://sites.google.com/view/hfr-challenge18/home
				%h4{class: 'specsess-heading'} Organizers
				= render 'chairs', target: :gc_hfr

	%div{class: 'panel panel-primary'}
		%div{class: 'panel-heading'}
			%h4{class: 'panel-title'}
				%a{'data-toggle': 'collapse', 'data-parent': '#accordion', href: '#gc-abr'}
					ABR Bandwidth Estimation For HTTP Chunked Transfer Coding
				%span{class: 'glyphicon glyphicon-link pull-right clickable-workshop-link', ldata: 'gc-abr'}
		%div{id: 'gc-abr', class: 'panel-collapse collapse'}
			%div{class: 'panel-body'}
				%h4{class: 'specsess-heading'} Description
				%p 
					One of the most successful efforts to reduce the latency of HTTP streaming is to utilize the HTTP chunked transfer coding, which enables a video segment to be generated and transmitted concurrently. For more technical details, please read Twitter’s recent technical blog 
					%a{href: 'https://medium.com/@periscopecode/introducing-lhls-media-streaming-eb6212948bef', target: '_medium'} 
						Introducing LHLS Media Streaming
						%span{class: 'glyphicon glyphicon-new-window'}


				%p However, compared with segment-based HTTP download, chunked transfer coding makes the bandwidth estimation a lot harder for any ABR playback algorithm. This Grand Challenge is to call for signal-processing/machine-learning algorithms that can effectively estimate download bandwidth based on the noisy samples of chunked-based download throughput.
				/%h4{class: 'specsess-heading'} Important Dates
				/ %ul 
				/ 	%li February 15, 2018: Results due
				/ 	%li February 25, 2018: Winner selected
				/ 	%li March 1, 2018: Winner paper submission due
				/ 	%li March 31, 2018: Paper acceptance notified
				/ 	%li April 13, 2018: Camera-ready paper submission due
				%h4{class: 'specsess-heading'} Website
				%a{href: 'https://blog.twitch.tv/twitch-invites-you-to-take-on-the-icme-2018-grand-challenge-2b3824d3537b'} https://blog.twitch.tv/twitch-invites-you-to-take-on-the-icme-2018-grand-challenge-2b3824d3537b
				%h4{class: 'specsess-heading'} Organizers
				= render 'chairs', target: :gc_abr

	%div{class: 'panel panel-primary'}
		%div{class: 'panel-heading'}
			%h4{class: 'panel-title'}
				%a{'data-toggle': 'collapse', 'data-parent': '#accordion', href: '#gc-dlfr'}
					Densely-sampled Light Field Reconstruction
				%span{class: 'glyphicon glyphicon-link pull-right clickable-workshop-link', ldata: 'gc-dlfr'}
		%div{id: 'gc-dlfr', class: 'panel-collapse collapse'}
			%div{class: 'panel-body'}
				%h4{class: 'specsess-heading'} Description
				%p Densely-sampled light field (DSLF) is a discrete representation of the 4D approximation of the plenoptic function, where multi-perspective camera views are arranged in such a way that the disparities between adjacent views are less than one pixel. DSLF is an attractive representation of scene visual content, particularly for applications which require ray interpolation and view synthesis. However, direct DSLF capture of real-world scenes is not practical. In this Grand Challenge, proponents are invited to develop and implement algorithms for DSLF reconstruction from decimated-parallax imagery, i.e. from a given sparse set of camera images.
				/%h4{class: 'specsess-heading'} Important Dates
				/ %ul
				/ 	%li December 15, 2017: Grand Challenge web site operational; release of Development Datasets and scripts for calculating the quality metric
				/ 	%li March 16, 2018: Submission of binaries to organizers
				/ 	%li March 23, 2018: Grand Challenge papers submission
				/ 	%li March 26, 2018: Feedback to proponents regarding running their binaries
				/ 	%li April 2, 2018: Grand Challenge acceptance notification
				/ 	%li April 13, 2018: Grand Challenge camera-ready paper submission
				/ 	%li April 30, 2018: Submission of corrected binaries
				%h4{class: 'specsess-heading'} Website
				%a{href: 'http://www.tut.fi/civit/index.php/icme-2018-grand-challenge-densely-sampled-light-field-reconstruction/'} http://www.tut.fi/civit/index.php/icme-2018-grand-challenge-densely-sampled-light-field-reconstruction/
				%h4{class: 'specsess-heading'} Organizers
				= render 'chairs', target: :gc_dlfr

	%div{class: 'panel panel-primary'}
		%div{class: 'panel-heading'}
			%h4{class: 'panel-title'}
				%a{'data-toggle': 'collapse', 'data-parent': '#accordion', href: '#gc-dash'}
					Grand Challenge on DASH
				%span{class: 'glyphicon glyphicon-link pull-right clickable-workshop-link', ldata: 'gc-dash'}
		%div{id: 'gc-dash', class: 'panel-collapse collapse'}
			%div{class: 'panel-body'}
				%h4{class: 'specsess-heading'} Description
				%p The MPEG DASH standard provides an interoperable representation format but deliberately does not define the adaptation behavior for the client implementations. In a typical deployment, the encoding is optimized for the respective delivery channels, but various issues during streaming (e.g., high startup delay, stalls/re-buffering, high switching frequency, inefficient network utilization, unfairness to competing network traffic, etc.) may limit the viewer experience.

				%p The goal of this grand challenge is to solicit contributions addressing end-to- end delivery aspects that will help improve the QoE while optimally using the network resources at an acceptable cost. Such aspects include, but are not limited to, content preparation for adaptive streaming, delivery in the Internet and streaming client implementations.

				%p A special focus of 2018’s grand challenge will be related to immersive media applications and services including omnidirectional/360-degree videos.
				%h4{class: 'specsess-heading'} Website
				%a{href: 'https://github.com/Dash-Industry-Forum/Academic-Track/wiki/DASH-Grand-Challenge-at-IEEE-ICME-2018'} https://github.com/Dash-Industry-Forum/Academic-Track/wiki/DASH-Grand-Challenge-at-IEEE-ICME-2018
				%h4{class: 'specsess-heading'} Organizers
				= render 'chairs', target: :gc_dash

	%div{class: 'panel panel-primary'}
		%div{class: 'panel-heading'}
			%h4{class: 'panel-title'}
				%a{'data-toggle': 'collapse', 'data-parent': '#accordion', href: '#gc-salient'}
					Salient360! 2018: Visual attention modeling for 360 Images - 2018 edition
				%span{class: 'glyphicon glyphicon-link pull-right clickable-workshop-link', ldata: 'gc-salient'}
		%div{id: 'gc-salient', class: 'panel-collapse collapse'}
			%div{class: 'panel-body'}
				%h4{class: 'specsess-heading'} Description
				%p Recent VR/AR applications still face important challenges. Particularly, understanding how users watch and explore 360° content and modelling visual attention is a key tech to develop appropriate rendering, coding and streaming techniques to create a good experience for consumers.

				%p 
					Salient360! 2018 is the follow-up of ICME’17 Salient360! Grand challenge. The first edition set the baseline for several types of visual attention models for 360° images, and ad-hoc methodologies and ground-truth data to test each type of model. With this second edition, it is expected to:
					%ol
						%li consolidate and improve the existing modeling.
						%li extend the type of models.
						%li extend the type of input contents.
				/%h4{class: 'specsess-heading'} Important Dates
				/ %ul
				/ 	%li March 1, 2018: Grand Challenge Winner Paper submission
				/ 	%li March 31, 2018: Grand Challenge Acceptance notification
				/ 	%li April 13, 2018: Grand Challenge Camera Ready Paper submission
				%h4{class: 'specsess-heading'} Website
				%a{href: 'https://salient360.ls2n.fr'} https://salient360.ls2n.fr
				%h4{class: 'specsess-heading'} Organizers
				= render 'chairs', target: :gc_salient

%h4{class: 'specsess-heading'} Submission Instructions
%p 
	Please use the online submission system 
	%a{href: 'https://cmt3.research.microsoft.com/User/Login?ReturnUrl=%2FICME2018W'} here				

= render 'deadline', target: [:grand_challenge_suggested_winner_paper, :grand_challenge_suggested_acceptance, :grand_challenge_suggested_camera_ready_paper]
= render 'chairs', target: :grand_challenge


:javascript
	layout_helper.activate_hotlinked();