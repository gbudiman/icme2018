
%h1 Industry Forum

%h3 Panels
%div{class: 'panel-group', id: 'accordion-industry'}
	%div{class: 'panel panel-primary'}
		%div{class: 'panel-heading'}
			%h4{class: 'panel-title'}
				%a{'data-toggle': 'collapse', 'data-parent': '#accordion-industry', href: '#collapse-1'}
					5G-enabled Multimedia User Experience
		%div{id: 'collapse-1', class: 'panel-collapse collapse'}
			%div{class: 'panel-body'}
				%h4{class: 'specsess-heading'} Synopsis
				%p 
					TBA
				%h4{class: 'specsess-heading'} Panelists
				= render 'chairs', target: :industry_a_panelists

	%div{class: 'panel panel-primary'}
		%div{class: 'panel-heading'}
			%h4{class: 'panel-title'}
				%a{'data-toggle': 'collapse', 'data-parent': '#accordion-industry', href: '#collapse-2'}
					XR: virtual, augmented and mixed reality
		%div{id: 'collapse-2', class: 'panel-collapse collapse'}
			%div{class: 'panel-body'}
				%h4{class: 'specsess-heading'} Synopsis
				%p 
					TBA
				%h4{class: 'specsess-heading'} Panelists
				%img{src: "#{asset_path('photos/Jill Boyce.jpg')}", class: 'keynote-bio'}
				%p
					Jill M. Boyce is Intel Fellow and Chief Media Architect at Intel. She represents Intel at the Joint Collaborative Team on Video Coding (JCT-VC) and Joint Video Exploration Team (JVET) of ITU-T SG16 and ISO/IEC MPEG. She serves as Associate Rapporteur of ITU-T VCEG, and was an editor of the Scalability High Efficiency Video Coding extension (SHVC).
				%p
					She received a B.S. in Electrical Engineering from the University of Kansas in 1988 and an M.S.E. in Electrical Engineering from Princeton University in 1990. She was formerly Director of Algorithms at Vidyo, Inc. where she led video and audio coding and processing algorithm development. She was formerly VP of Research and Innovation Princeton for Technicolor, formerly Thomson. She was formerly with Lucent Technologies Bell Labs, AT&T Labs, and Hitachi America. She was Associate Editor from 2006 to 2010 of IEEE Transactions on Circuits and Systems for Video Technology. She is the inventor of more than 150 granted U.S. patents, and has published more than 40 papers in peer-reviewed conferences and journals.

				%hr
				%img{src: "#{asset_path('photos/Phillip Chou.jpg')}", class: 'keynote-bio'}
				%p
					Philip A. Chou has longstanding interests in data compression, signal processing, machine learning, communication theory, and information theory and their applications to processing media such as dynamic point clouds, video, images, audio, speech, and documents.  He did the first work on multiple reference frame video coding, he originated rate-distortion optimization for codecs, and he performed the seminal work on client-driven network-adaptive streaming media on demand leading up to Microsoft IIS Smooth Streaming and subsequent standards.  He is one of the inventors of practical network coding using random codes, and one of the inventors of wireless network coding.  He holds degrees in electrical engineering and computer science from Princeton, Berkeley, and Stanford.  He has been a member of the research staff or research manager at AT&T Bell Laboratories, Xerox PARC, and Microsoft Research.  He has played key roles in startups Telesensory Systems, Speech Plus, VXtreme (acquired by Microsoft), and 8i.  He has been an affiliate faculty member at Stanford, the University of Washington, and the Chinese University of Hong Kong.  He has been associate or guest editor for the IEEE Trans. Information Theory, the IEEE Trans. Image Processing, and the IEEE Trans. Multimedia.  He has been an organizer or technical co-chair for the inaugural NetCod, ICASSP’07, MMSP’09, ICIP’15, ICME’16, ICIP’17, among others.  He is an IEEE Fellow and has served on the IEEE Fellow evaluation committees of the IEEE Computer and Signal Processing societies, as well as on the Board of Governors of the IEEE Signal Processing Society.  He has been an active participant in MPEG, where he instigated the work on the file format, and contributed algorithms and code used for static point cloud compression.  He has won or co-authored best paper awards in the IEEE Trans. Signal Processing, the IEEE Trans. Multimedia, ICME, and ICASSP.  He is a Fellow of the IEEE.  He is co-editor of a book on multimedia communication.  He is currently with 8i.com, a startup spread across Wellington, Los Angeles, and Seattle, where he leads the effort to compress and communicate volumetric video, popularly known as holograms, for virtual and augmented reality.

				%hr
				%img{src: "#{asset_path('photos/Serafin Diaz.jpg')}", class: 'keynote-bio'}
				%p
					Serafin Diaz is a Vice President of engineering at Qualcomm and currently leads XR Research. His experience prior to joining Qualcomm includes digital hardware designer, LAN systems engineer, software architect, and formal tester for TDMA cellular systems.
				%p
					After joining Qualcomm in 1997, Serafin led a variety of projects in areas concerning 2G CDMA data systems, test automation platforms for wireless devices, physical layer system integration and test for 1xEVDO as well as projects which laid down core technology for wireless VoIP and Video Telephony systems. 
				%p
					Serafin co-founded and led the first Augmented Reality project in Qualcomm in 2007.  Such project produced core Real Time Computer Vision technology which has enabled not only Augmented Reality but also became foundation to projects in Robotics, Automotive, and Virtual Reality. The inside-out 6DoF head tracking technology recently showcased in Qualcomm’s VR reference design HMD is an example of such technology.
				%p
					Serafin received his undergraduate degree in electronic systems from the ITESM, Monterrey, Mexico in 1989 and his Master’s degree in electrical engineering from SMU, Dallas, USA in 1994. 

				%hr
				%img{src: "#{asset_path('photos/Jon Karafin.jpg')}", class: 'keynote-bio'}
				%p 
					Jon Karafin has dedicated his career to innovation in live action cinema, VFX post-production, and light field technology – transforming bleeding-edge concepts into market ready solutions. As CEO of Light Field Lab, he applies this expertise to the development of a next-generation holographic technology.

				%p
					Karafin has an extensive background in light field and visual effects technology, having previously served as Head of Light Field at Lytro, Vice President of Production Technology at RealD, and as Director of Production, Technology, and Operations at Digital Domain. During his tenure, he was responsible for ushering in a new era of cinematic capture through the launch of Lytro Cinema, as well as delivering technology and content for many of the all-time highest grossing feature films, including Peter Jackson’s The Hobbit, Michael Bay’s Transformers 3, and Tim Burton’s Alice in Wonderland.

				%p
					Karafin holds multiple graduate degrees from the Rochester Institute of Technology (RIT), as well as BFAs in multiple fields from Ithaca College.
				%hr
				%img{src: "#{asset_path('photos/Jens-Rainer Ohm.jpg')}", class: 'keynote-bio'}
				%p
					Jens-Rainer Ohm holds the chair position of the Institute of Communication Engineering at RWTH Aachen University, Germany since 2000. His research and teaching activities cover the areas of multimedia signal processing, analysis, compression, transmission and content description, including 3D and VR video applications, bio signal processing and communication, application of deep learning approaches in the given fields, as well as fundamental topics of signal processing and digital communication systems.

				%p
					Since 1998, he participates in the work of the Moving Picture Experts Group (MPEG). He has been chairing/co-chairing various standardization activities in video coding, namely the MPEG Video Subgroup 2002-2018, the Joint Video Team (JVT) of MPEG and ITU-T SG 16 VCEG 2005-2009, the Joint Collaborative Team on Video Coding (JCT-VC) since 2010, as well as the Joint Video Experts Team (JVET) since 2015.

				%p
					Prof. Ohm has authored textbooks on multimedia signal processing, analysis and coding, on communication engineering and signal transmission, as well as numerous papers from the fields mentioned above.







%h3 Industrial Plenary Talks
%div{class: 'panel-group', id: 'accordion-plenary'}
	%div{class: 'panel panel-primary'}
		%div{class: 'panel-heading'}
			%h4{class: 'panel-title'}
				%a{'data-toggle': 'collapse', 'data-parent': '#accordion-plenary', href: '#collapse-3'}
					InterDigital: 5G is Here - Is it time to celebrate?
		%div{id: 'collapse-3', class: 'panel-collapse collapse'}
			%div{class: 'panel-body'}
				%h4{class: 'specsess-heading'} Abstract
				%p 
					The widely anticipated 5G cellular specifications, 3GPP Release 15, are here. Deployments are starting, devices will appear soon, and there’s plenty of buzz about who’s first, who’s best and what is to come.  5G brings great promises of 20 Gbps data rates, 1 ms latency, long battery life, and network enhancements: a Service Based Architecture, Network Function Virtualization, and Network Slicing.  But what does it all mean and what is to come? Are we overly enthusiastic, or are those who are ambivalent or skeptical justified? 
				%p 
					This talk will take a brief look at the evolution of cellular standards, the expectations, the successes, and the failures. It will then focus on how 5G is different and discuss how success will follow from leveraging the flexible 5G technologies for a larger ecosystem that can benefit from the broadband continuous coverage of cellular networks. Advanced multimedia services are one of the most important use cases. Yet, success may also depend on high performance localized applications using mobile edge computing, IoT, new entrants operating in unlicensed spectrum, contributions to the automobile industry’s plans for autonomous and assisted driving, non-terrestrial networks offering the ability to integrate satellite systems, unmanned aerial vehicles, robotics, and as history shows, those yet-to-be-imagined applications.

				%h4{class: 'specsess-heading'} About the Speaker
				%img{src: "#{asset_path('photos/Robert DiFazio.jpg')}", class: 'keynote-bio'}
				%p
					Dr. Robert A. DiFazio, Head of Research &amp; Development, Vice President, InterDigital Labs, InterDigital Communications, Inc. Dr. Robert A. DiFazio is the Head of Research &amp; Development and Vice President of InterDigital Labs, where he leads a group of engineers who design and develop advanced technologies and applications for mobile communications. He manages and actively participates in numerous projects addressing 5G cellular technology, next generation Wi-Fi, millimeter wave radio systems, small cell and heterogeneous wireless networks, advanced video standards and platforms, emerging network technology, IoT and machine-to- machine communications, and advanced sensor systems for navigation and localization. He contributes to technology planning at InterDigital and the company’s collaboration with many universities. Dr. DiFazio has almost forty years of experience in research, design, implementation, and testing of new technologies for commercial and military wireless systems. Prior to InterDigital, he spent more than twenty years at BAE Systems working on software defined radios, smart antenna systems, jam resistant modems, and low probability of intercept communication and navigation systems. He has a Ph.D. from the NYU Tandon School of Engineering (formerly, Brooklyn Poly). He serves on the Industry Advisory Boards for the NYU Tandon Department of Electrical Engineering and Computer Science and for New York Institute of Technology. He is a Senior Member of the IEEE and holds over forty issued and numerous pending US patents.

	%div{class: 'panel panel-primary'}
		%div{class: 'panel-heading'}
			%h4{class: 'panel-title'}
				%a{'data-toggle': 'collapse', 'data-parent': '#accordion-plenary', href: '#collapse-4'}
					Tencent: Neural Network in Video Compression and Standard
		%div{id: 'collapse-4', class: 'panel-collapse collapse'}
			%div{class: 'panel-body'}
				%h4{class: 'specsess-heading'} Synopsis
				%p 
					HEVC (High Efficiency Video Coding) has emerged as a major step forward in video compression and standardization. This achievement was recognized by the Emmy Engineering Award in October 2017.  At the same time new video compression technologies continue being actively developed beyond HEVC to suit the rapidly growing market demands. A Call for Proposals was jointly issued by ISO/IEC and ITU-T in October 2017 to launch a new standardization project to capture these advances. More than 40 responses were received in April 2018, among which some new elements were presented besides more conventional video coding techniques, including the utilization of neural networks for video compression. Neural network or deep learning technologies have been researched for enhancing video and image qualities, and more recently, video and image compression. This talk will look into the recent work on neural video compression for the next video compression standard and discuss the opportunities as well as challenges.
				%h4{class: 'specsess-heading'} Panelists
				%img{src: "#{asset_path('photos/Shan Liu.jpg')}", class: 'keynote-bio'}
				%p 
					Shan Liu is a Distinguished Scientist and Vice President of Tencent Media Lab at Tencent America. Prior to Tencent she was the Chief Scientist and Head of America Media Lab at Futurewei Technologies, a.k.a. Huawei USA. She also held senior management and technical positions at MediaTek, Mitsubishi Electric Research Laboratories, Sony Electronics / Sony Computer Entertainment America, and IBM T.J. Watson Research Center. Dr. Liu is the inventor of more than 200 US and global patent applications and the author of more than 30 journal and conference articles. Many of her inventions have been adopted by international standards such as ITU-T H.265 | ISO/IEC HEVC, MPEG-DASH and OMAF, as well as utilized in widely sold commercial products. She has chaired and co-chaired a number of ad-hoc and technical groups through standard development and served as co-Editor of Rec. ITU-T H.265 v4 | ISO/IEC 23008-2:2017. She has been in technical and organizing committees, or an invited speaker, at various international conferences such as IEEE ICIP, VCIP, ICNC, ICME and ACM Multimedia. She served in Industrial Relationship Committee of IEEE Signal Processing Society 2014-2015 and was appointed the VP of Industrial Relations and Development of Asia-Pacific Signal and Information Processing Association (APSIPA) 2016-2017. Dr. Liu obtained her B.Eng. degree in Electronics Engineering from Tsinghua University, Beijing, China and M.S. and Ph.D. degrees in Electrical Engineering from University of Southern California, Los Angeles, USA.

= render 'chairs', target: :industrial_program